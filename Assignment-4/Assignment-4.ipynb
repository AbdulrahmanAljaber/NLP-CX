{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Classification\n",
    "\n",
    "In this assignment you are required to perform text classification on App review dataset consisting of 4 classes:\n",
    "- Bug reports\n",
    "- Feature\n",
    "- Rating\n",
    "- UserExperience\n",
    "\n",
    "There are a total of 3733 samples. You need to:\n",
    "- Split the data into train/validate/test sets (70/15/15) using random seed '777' with shuffling.\n",
    "- You need to investigate issues of stop-words, infrequent words, text normalization (stemming, lemmatization, other issues of word tokenization like case normalization, punctuations). Additionally, you can also apply techniques to solve data imbalance problem. \n",
    "- You need to report appropriate measures like accuracy, precision, recall, and f1 scores (you can classification report api)\n",
    "- You should show the confusion matrix for the validation and test sets\n",
    "\n",
    "***Note:***\n",
    "- Student getting the best macro-average F1-Score receives 15% bonus grades\n",
    "- Student getting the second-best macro-average F1-Score receives 10% bonus grades\n",
    "- Student getting the third-best macro-average F1-Score receives 5% bonus grades"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import log_loss, f1_score, recall_score, confusion_matrix, classification_report\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import multilabel_confusion_matrix, confusion_matrix\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from tqdm import tqdm\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from tensorflow import keras\n",
    "from keras.layers import (LSTM, \n",
    "                          Embedding, \n",
    "                          BatchNormalization,\n",
    "                          Dense, \n",
    "                          TimeDistributed, \n",
    "                          Dropout, \n",
    "                          Bidirectional,\n",
    "                          Flatten, \n",
    "                          GlobalMaxPool1D)\n",
    "from keras.initializers import Constant\n",
    "from keras.optimizers import adam_v2\n",
    "from nltk.tokenize import word_tokenize\n",
    "import transformers\n",
    "import nltk\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Read the dataset and split it into different sets\n",
    "**[20 points]** for invesitaging issues of stop-words, infrequent words, text normalization (stemming, lemmatization, other issues of word tokenization), dataset imbalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Review</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Besides the occasional crash, this is an amazi...</td>\n",
       "      <td>Bug</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>This could be a great app if it was predictabl...</td>\n",
       "      <td>Bug</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I can't open since the last 2 updates Pop-ups ...</td>\n",
       "      <td>Bug</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Use to love this app but it's not working afte...</td>\n",
       "      <td>Bug</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Urrrrm\\tAfter my third re installing, it final...</td>\n",
       "      <td>Bug</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Review label\n",
       "0  Besides the occasional crash, this is an amazi...   Bug\n",
       "1  This could be a great app if it was predictabl...   Bug\n",
       "2  I can't open since the last 2 updates Pop-ups ...   Bug\n",
       "3  Use to love this app but it's not working afte...   Bug\n",
       "4  Urrrrm\\tAfter my third re installing, it final...   Bug"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('AppReviews-FourClasses.csv') # read data \n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Review</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>3733</td>\n",
       "      <td>3733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>3217</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>Good</td>\n",
       "      <td>Rating</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>15</td>\n",
       "      <td>2461</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Review   label\n",
       "count    3733    3733\n",
       "unique   3217       4\n",
       "top      Good  Rating\n",
       "freq       15    2461"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Review    0\n",
       "label     0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.isnull().sum() # checking for null values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### removing stopwords and punc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = data['Review']\n",
    "punctuation = '\"#$%&()+,-./:;<=>@[\\]^_`{|}~'\n",
    "\n",
    "def text_cleaning(data):\n",
    "    sw_file = open(\"stopwords.txt\", \"r\")\n",
    "    content = sw_file.read()\n",
    "    stopwords = content.split(\"\\n\")\n",
    "    \n",
    "    remove_punc = [i for i in data if i  not in punctuation]\n",
    "    remove_punc = ''.join(remove_punc)\n",
    "    \n",
    "    remove_stopwords = [i for i in remove_punc.split() if i.lower() not in stopwords]\n",
    "    processed = ' '.join(remove_stopwords)\n",
    "    \n",
    "    return processed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Review</th>\n",
       "      <th>label</th>\n",
       "      <th>cleaned</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Besides the occasional crash, this is an amazi...</td>\n",
       "      <td>Bug</td>\n",
       "      <td>Besides occasional crash amazing product tons ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>This could be a great app if it was predictabl...</td>\n",
       "      <td>Bug</td>\n",
       "      <td>could great app predictable full bugs unpredic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I can't open since the last 2 updates Pop-ups ...</td>\n",
       "      <td>Bug</td>\n",
       "      <td>can't open since last 2 updates Popups go craz...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Use to love this app but it's not working afte...</td>\n",
       "      <td>Bug</td>\n",
       "      <td>Use love app it's working new update Pages won...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Urrrrm\\tAfter my third re installing, it final...</td>\n",
       "      <td>Bug</td>\n",
       "      <td>Urrrrm third re installing finally scenery han...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Review label  \\\n",
       "0  Besides the occasional crash, this is an amazi...   Bug   \n",
       "1  This could be a great app if it was predictabl...   Bug   \n",
       "2  I can't open since the last 2 updates Pop-ups ...   Bug   \n",
       "3  Use to love this app but it's not working afte...   Bug   \n",
       "4  Urrrrm\\tAfter my third re installing, it final...   Bug   \n",
       "\n",
       "                                             cleaned  \n",
       "0  Besides occasional crash amazing product tons ...  \n",
       "1  could great app predictable full bugs unpredic...  \n",
       "2  can't open since last 2 updates Popups go craz...  \n",
       "3  Use love app it's working new update Pages won...  \n",
       "4  Urrrrm third re installing finally scenery han...  "
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['cleaned'] = data.Review.apply(text_cleaning)\n",
    "data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "le.fit(data['label'])\n",
    "\n",
    "data['label_encoded'] = le.transform(data['label'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming and Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Review</th>\n",
       "      <th>label</th>\n",
       "      <th>cleaned</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Besides the occasional crash, this is an amazi...</td>\n",
       "      <td>Bug</td>\n",
       "      <td>besid occas crash amaz product ton potenti dep...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>This could be a great app if it was predictabl...</td>\n",
       "      <td>Bug</td>\n",
       "      <td>could great app predict full bug unpredict abl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I can't open since the last 2 updates Pop-ups ...</td>\n",
       "      <td>Bug</td>\n",
       "      <td>can't open sinc last 2 updat popup go crazi ip...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Use to love this app but it's not working afte...</td>\n",
       "      <td>Bug</td>\n",
       "      <td>use love app it work new updat page won't scro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Urrrrm\\tAfter my third re installing, it final...</td>\n",
       "      <td>Bug</td>\n",
       "      <td>urrrrm third re instal final sceneri hand blac...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Review label  \\\n",
       "0  Besides the occasional crash, this is an amazi...   Bug   \n",
       "1  This could be a great app if it was predictabl...   Bug   \n",
       "2  I can't open since the last 2 updates Pop-ups ...   Bug   \n",
       "3  Use to love this app but it's not working afte...   Bug   \n",
       "4  Urrrrm\\tAfter my third re installing, it final...   Bug   \n",
       "\n",
       "                                             cleaned  \n",
       "0  besid occas crash amaz product ton potenti dep...  \n",
       "1  could great app predict full bug unpredict abl...  \n",
       "2  can't open sinc last 2 updat popup go crazi ip...  \n",
       "3  use love app it work new updat page won't scro...  \n",
       "4  urrrrm third re instal final sceneri hand blac...  "
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmer = nltk.SnowballStemmer(\"english\")\n",
    "\n",
    "def stemm_text(text):\n",
    "    text = ' '.join(stemmer.stem(word) for word in text.split(' '))\n",
    "    return text\n",
    "\n",
    "data['cleaned'] = data['cleaned'].apply(stemm_text)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lemmatizer=nltk.stem.WordNetLemmatizer()\n",
    "\n",
    "# def lemma_text(text):\n",
    "#     text = ' '.join(lemmatizer.lemmatize(word) for word in text.split(' '))\n",
    "#     return text\n",
    "\n",
    "# data['cleaned'] = data['cleaned'].apply(lemma_text)\n",
    "# data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### label encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lbl_enc = preprocessing.LabelEncoder()\n",
    "y = data.label_encoded.values\n",
    "# print(y)\n",
    "\n",
    "X = data.cleaned.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainx shape: (2613,)\n",
      "trainy shape: (2613,)\n",
      "valx shape: (560,)\n",
      "valy shape: (560,)\n",
      "testx shape: (560,)\n",
      "testy shape: (560,)\n"
     ]
    }
   ],
   "source": [
    "trainx, valx, trainy, valy = train_test_split(X,y,test_size=0.30,random_state=777)\n",
    "testx, valx, testy, valy = train_test_split(valx,valy,test_size=0.50,random_state=777)\n",
    "\n",
    "print('trainx shape:',trainx.shape)\n",
    "print('trainy shape:',trainy.shape)\n",
    "print('valx shape:',valx.shape)\n",
    "print('valy shape:',valy.shape)\n",
    "print('testx shape:',testx.shape)\n",
    "print('testy shape:',testy.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Balancing the data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Rating            2461\n",
       "UserExperience     607\n",
       "Bug                370\n",
       "Feature            295\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 254,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from imblearn.under_sampling import NearMiss\n",
    "from collections import Counter\n",
    "\n",
    "undersample = NearMiss(version=1, n_neighbors=3)\n",
    "# transform the dataset\n",
    "trainx, trainy = undersample.fit_resample(trainx, trainy)\n",
    "counter = Counter(trainy)\n",
    "print(counter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. [15 points] Perform text classification using bag-of-words features\n",
    "In this part, you can use any classifier of your choice such as logistic regression or neural networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vectorizer = CountVectorizer()\n",
    "trainx_bow = count_vectorizer.fit_transform(trainx)\n",
    "valx_bow = count_vectorizer.transform(valx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BoW log_loss accuracy : 0.8638643545734025\n"
     ]
    }
   ],
   "source": [
    "bag_logistic = BaggingClassifier(LogisticRegression(C=1.0,max_iter = 10000))\n",
    "bag_logistic.fit(trainx_bow, trainy)\n",
    "\n",
    "logistic_pred = bag_logistic.predict_proba(valx_bow)\n",
    "print('BoW log_loss accuracy :' ,log_loss(valy, logistic_pred))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. [15 points] Perform text classification using tf-idf features\n",
    "In this part, you can use any classifier of your choice such as logistic regression or neural networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tfv = TfidfVectorizer(min_df=3,  max_features=None, \n",
    "            strip_accents='unicode', analyzer='word',token_pattern=r'\\w{1,}',\n",
    "            ngram_range=(1, 3), use_idf=1,smooth_idf=1,sublinear_tf=1,\n",
    "            stop_words = 'english')\n",
    "\n",
    "trainx_tfv = tfv.fit_transform(trainx)\n",
    "test_tfv =  tfv.transform(testx) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf-idf log_loss accuracy : 0.7591844526629771\n"
     ]
    }
   ],
   "source": [
    "bag_logistic = BaggingClassifier(LogisticRegression(C=1.0,max_iter = 10000))\n",
    "bag_logistic.fit(trainx_tfv, trainy)\n",
    "\n",
    "logistic_pred = bag_logistic.predict_proba(test_tfv)\n",
    "print('tf-idf log_loss accuracy :' ,log_loss(testy, logistic_pred))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. [20 points] Perform text classification using dense vectors like word2vec or Glove embeddings\n",
    "In this part, you can use any classifier of your choice such as logistic regression or neural networks. You can download and use precomputed embeddings or create your own word2vec style embeddings using libraries such as ```gensim``` (from gensim.models import Word2Vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3733/3733 [00:00<00:00, 10905.84it/s]\n"
     ]
    }
   ],
   "source": [
    "def create_corpus_new(df):\n",
    "    corpus=[]\n",
    "    for tweet in tqdm(df['cleaned']):\n",
    "        words=[word.lower() for word in word_tokenize(tweet)]\n",
    "        corpus.append(words)\n",
    "    return corpus \n",
    "corpus=create_corpus_new(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dict={}\n",
    "with open('glove.6B.100d.txt','r') as f:\n",
    "    for line in f:\n",
    "        values=line.split()\n",
    "        word = values[0]\n",
    "        vectors=np.asarray(values[1:],'float32')\n",
    "        embedding_dict[word]=vectors\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN=50\n",
    "tokenizer_obj=Tokenizer()\n",
    "tokenizer_obj.fit_on_texts(corpus)\n",
    "sequences=tokenizer_obj.texts_to_sequences(corpus)\n",
    "\n",
    "text_pad=pad_sequences(sequences,maxlen=MAX_LEN,truncating='post',padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique words: 4599\n"
     ]
    }
   ],
   "source": [
    "word_index=tokenizer_obj.word_index\n",
    "print('Number of unique words:',len(word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4599/4599 [00:00<00:00, 706915.53it/s]\n"
     ]
    }
   ],
   "source": [
    "num_words=len(word_index)+1\n",
    "embedding_matrix=np.zeros((num_words,100))\n",
    "\n",
    "for word,i in tqdm(word_index.items()):\n",
    "    if i < num_words:\n",
    "        emb_vec=embedding_dict.get(word)\n",
    "        if emb_vec is not None:\n",
    "            embedding_matrix[i]=emb_vec "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_pad[0][0:]\n",
    "train_glove =text_pad[:text.shape[0]]\n",
    "test=text_pad[text.shape[0]:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of train (3173, 50)\n",
      "Shape of Validation  (560, 50)\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "X_train,X_val,y_train,y_val=train_test_split(train_glove,data['label'].values,test_size=0.15,random_state=777)\n",
    "print('Shape of train',X_train.shape)\n",
    "print(\"Shape of Validation \",X_val.shape)\n",
    "print(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf-idf log_loss accuracy : 0.9844802437020294\n"
     ]
    }
   ],
   "source": [
    "bag_logistic = BaggingClassifier(LogisticRegression(C=1.0,max_iter = 10000))\n",
    "bag_logistic.fit(X_train,y_train)\n",
    "\n",
    "logistic_pred = bag_logistic.predict_proba(X_val)\n",
    "print('Glove log_loss accuracy :' ,log_loss(y_val, logistic_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. [20 points] Perform text classification using learnt embeddings\n",
    "Here you should use RNNs. Here you need to start with random embedding vectors that will be learnt together with the main task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "\n",
    "def tokenization(text):\n",
    "    num_words = 1000\n",
    "    pad_type = 'post'\n",
    "    trunc_type = 'post'\n",
    "    tokenizer = Tokenizer(num_words=num_words)\n",
    "    tokenizer.fit_on_texts(text)\n",
    "\n",
    "    word_index = tokenizer.word_index\n",
    "\n",
    "\n",
    "    train_sequences = tokenizer.texts_to_sequences(data['Review'])\n",
    "\n",
    "\n",
    "    maxlen = max([len(x) for x in train_sequences])\n",
    "\n",
    "\n",
    "    train_padded = pad_sequences(train_sequences, maxlen=maxlen, padding=pad_type)\n",
    "\n",
    "\n",
    "    print(len(word_index))\n",
    "    print(train_padded.shape)\n",
    "\n",
    "\n",
    "    print(\"\\nPadded training sequences:\\n\", train_padded)\n",
    "    print(\"\\nPadded training shape:\", train_padded.shape)\n",
    "    print(\"Training sequences data type:\", type(train_sequences))\n",
    "    print(\"Padded Training sequences data type:\", type(train_padded))\n",
    "    return maxlen,train_sequences,train_padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5923\n",
      "(3733, 230)\n",
      "\n",
      "Padded training sequences:\n",
      " [[  1 396   8 ...   0   0   0]\n",
      " [  8  93  28 ...   0   0   0]\n",
      " [  2  63 131 ...   0   0   0]\n",
      " ...\n",
      " [ 48 237 562 ...   0   0   0]\n",
      " [  2  13   4 ...   0   0   0]\n",
      " [747 215  10 ...   0   0   0]]\n",
      "\n",
      "Padded training shape: (3733, 230)\n",
      "Training sequences data type: <class 'list'>\n",
      "Padded Training sequences data type: <class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "maxlen, tensor, X = tokenization(data['Review'])\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, tensor, test_size=0.30, random_state=777)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_23\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_23 (Embedding)    (None, 230, 230)          858590    \n",
      "                                                                 \n",
      " bidirectional_1 (Bidirectio  (None, 230, 460)         848240    \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " global_max_pooling1d_1 (Glo  (None, 460)              0         \n",
      " balMaxPooling1D)                                                \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 460)              1840      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 460)               0         \n",
      "                                                                 \n",
      " dense_20 (Dense)            (None, 230)               106030    \n",
      "                                                                 \n",
      " dropout_4 (Dropout)         (None, 230)               0         \n",
      "                                                                 \n",
      " dense_21 (Dense)            (None, 230)               53130     \n",
      "                                                                 \n",
      " dropout_5 (Dropout)         (None, 230)               0         \n",
      "                                                                 \n",
      " dense_22 (Dense)            (None, 1)                 231       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,868,061\n",
      "Trainable params: 1,867,141\n",
      "Non-trainable params: 920\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Embedding(input_dim=X.shape[0], output_dim=X.shape[1], weights = [X], input_length=maxlen))\n",
    "\n",
    "model.add(Bidirectional(LSTM(maxlen, return_sequences = True, recurrent_dropout=0.2)))\n",
    "\n",
    "model.add(GlobalMaxPool1D())\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(maxlen, activation = \"relu\"))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(maxlen, activation = \"relu\"))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1, activation = 'softmax'))\n",
    "model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checkpoint = ModelCheckpoint('model.h5', monitor = 'val_loss', verbose = 1, save_best_only = True)\n",
    "# reduce_lr = ReduceLROnPlateau(monitor = 'val_loss', factor = 0.2, verbose = 1, patience = 5,min_lr = 0.001)\n",
    "history = model.fit(X_train, \n",
    "    trainy, \n",
    "    epochs = 7,\n",
    "    batch_size = 32,\n",
    "    validation_data = (X_test, testy),\n",
    "    verbose = 1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. [BONUS: 15 points] Perform text classification using contextual embeddings such as BERT\n",
    "Here you should use RNNs or transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|██████████| 226k/226k [00:00<00:00, 466kB/s] \n",
      "Downloading: 100%|██████████| 28.0/28.0 [00:00<00:00, 18.6kB/s]\n",
      "Downloading: 100%|██████████| 571/571 [00:00<00:00, 570kB/s]\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-large-uncased')\n",
    "\n",
    "def bert_encode(data, maximum_length) :\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "\n",
    "    for text in data:\n",
    "        encoded = tokenizer.encode_plus(\n",
    "            text, \n",
    "            add_special_tokens=True,\n",
    "            max_length=maximum_length,\n",
    "            pad_to_max_length=True,\n",
    "\n",
    "            return_attention_mask=True,\n",
    "        )\n",
    "        input_ids.append(encoded['input_ids'])\n",
    "        attention_masks.append(encoded['attention_mask'])\n",
    "        \n",
    "    return np.array(input_ids),np.array(attention_masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\0xboja\\anaconda3\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2263: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "texts = data['cleaned']\n",
    "target = data['label_encoded']\n",
    "\n",
    "train_input_ids, train_attention_masks = bert_encode(texts,60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "def create_model(bert_model):\n",
    "    \n",
    "    input_ids = tf.keras.Input(shape=(60,),dtype='int32')\n",
    "    attention_masks = tf.keras.Input(shape=(60,),dtype='int32')\n",
    "\n",
    "    output = bert_model([input_ids,attention_masks])\n",
    "    output = output[1]\n",
    "    output = tf.keras.layers.Dense(32,activation='relu')(output)\n",
    "    output = tf.keras.layers.Dropout(0.2)(output)\n",
    "    output = tf.keras.layers.Dense(1,activation='sigmoid')(output)\n",
    "    \n",
    "    model = tf.keras.models.Model(inputs = [input_ids,attention_masks],outputs = output)\n",
    "    model.compile(Adam(lr=1e-5), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at bert-base-uncased were not used when initializing TFBertModel: ['mlm___cls', 'nsp___cls']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "from transformers import TFBertModel\n",
    "bert_model = TFBertModel.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_3 (InputLayer)           [(None, 60)]         0           []                               \n",
      "                                                                                                  \n",
      " input_4 (InputLayer)           [(None, 60)]         0           []                               \n",
      "                                                                                                  \n",
      " tf_bert_model_1 (TFBertModel)  TFBaseModelOutputWi  109482240   ['input_3[0][0]',                \n",
      "                                thPoolingAndCrossAt               'input_4[0][0]']                \n",
      "                                tentions(last_hidde                                               \n",
      "                                n_state=(None, 60,                                                \n",
      "                                768),                                                             \n",
      "                                 pooler_output=(Non                                               \n",
      "                                e, 768),                                                          \n",
      "                                 past_key_values=No                                               \n",
      "                                ne, hidden_states=N                                               \n",
      "                                one, attentions=Non                                               \n",
      "                                e, cross_attentions                                               \n",
      "                                =None)                                                            \n",
      "                                                                                                  \n",
      " dense_25 (Dense)               (None, 32)           24608       ['tf_bert_model_1[0][1]']        \n",
      "                                                                                                  \n",
      " dropout_81 (Dropout)           (None, 32)           0           ['dense_25[0][0]']               \n",
      "                                                                                                  \n",
      " dense_26 (Dense)               (None, 1)            33          ['dropout_81[0][0]']             \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 109,506,881\n",
      "Trainable params: 109,506,881\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\0xboja\\anaconda3\\lib\\site-packages\\keras\\optimizer_v2\\adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "model = create_model(bert_model)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "    [train_input_ids, train_attention_masks],\n",
    "    target,\n",
    "    validation_split=0.2, \n",
    "    epochs=3,\n",
    "    batch_size=10\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [10 points] Document your conclusions on:\n",
    "- General conclusions about the task\n",
    "- Related to text preprocessing \n",
    "- Related to using different features/embeddings\n",
    "- Effect of different hyperparameters\n",
    "\n",
    "Write bullet points based report assuming you are presenting your conclusions to the project manager."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Error analysis and possible improvements \n",
    "* The task was quit challanging and intreststing at the same time, hence it allowed us to explore and implment diffrent nlp taqnuiqes.\n",
    "* We have implemented diffrent tequnqes on the preprocessing phase, such as removing punc, stemming, lemmtization, balancing the data set, and these tqunqese helped improving the classfiers accurcy.\n",
    "* a shocking result is that the model that used Bag of word approach gave us the highest accuracy score.\n",
    "* by using bagClassfier boosting tequnqes were able to achive a 87% accuracy\n",
    "* having an  imbalanced dataset, and we dealt with it by making all the classes equal, and that improved our models accuracy by 3%\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
