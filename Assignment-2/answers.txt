================================================================================
================================================================================
Name: ABDULRHMAN ALJABER
NetID:
================================================================================
================================================================================
Submission Instructions (10 points)

Make sure to follow the submission instructions as specified in the assignment.
The submission zip file should be uploaded to NYU classes.

================================================================================
================================================================================
Task 1 : Let's LM!  (10 points)
--------------------------------------------------------------------------------
Question 1.a  (1 point)  What is the effect of this tokenization step?
Describe the changes in the text.  Hint: check out nonbreaking_prefix.en.
It added a space between words and symbols/digits.It also
--------------------------------------------------------------------------------
Question 1.b (2 points)  What is the difference between the raw and tokenized
files in terms of the number of tokens and the number of types (unique tokens)?
Explain the difference in the numbers given the used tokenization.

it added space between punctuation and digits

--------------------------------------------------------------------------------
Question 1.c  (1 point)  What is the total number of 1-gram, 2-gram and 3-gram
entries?

ngram 1=17862
ngram 2=174767
ngram 3=255058

--------------------------------------------------------------------------------
Question 1.d  (1 point)  What is the purpose of each column in the 1-grams data?

Each N-gram line starts with the logarithm (base 10) of conditional probability p of that N-gram, 
followed by the words w1...wN making up the N-gram. 
These are optionally followed by the logarithm (base 10) of the backoff weight for the N-gram. 



--------------------------------------------------------------------------------
Question 1.e  (1 point)  Why is there no third column in the 3-grams portion?

The third column is backoff weight for the n-gram and in backoff, we use the trigram if the evidence is
sufficient, otherwise we use the bigram, otherwise the unigram. In other words, we
only “back off” to a lower-order n-gram if we have zero evidence for a higher-order
n-gram.

--------------------------------------------------------------------------------
Question 1.f  (1 point)  Why is the UN LM perplexity different for the Bible
and UN?

the model was trained on UN corpus, and hince (the lower is the better) the UN test preformed better on the test.

--------------------------------------------------------------------------------
Question 1.g  (1 point)  Compute the OOV rate for the UN and Bible test files?
(% of OOVs/words).


OOV = 3053
words = 13999
% of OOVs/words = 3053/13999= 0.218 ~~22%

--------------------------------------------------------------------------------
Question 1.h (1 point) Why is the UN LM OOV rate different for the Bible and UN?


because we trained the model on UN corpus, and r out of vocabulary (OOV) also known as unknown words that we havent seen before.

--------------------------------------------------------------------------------
Question 1.i (1 point) Provide the text you randomly generated.
How is its fluency? How is it in terms of coherence?

coherence was good in the tri-gram

================================================================================
================================================================================
Task 2 : How Many Ways to LM? (40 points)

Question 2.1.a (4 points). Fill in the below table.



****************************************************************************
                           |  Words |   OOV   |  OOV%   |     ppl          |
****************************************************************************
UNCorpus.test:             |  16693 |  1891   |         |     28.17   |
----------------------------------------------------------------------------
UNCorpus.test.tok:         |   19169  	|  16     |         |     19.39789     |
----------------------------------------------------------------------------
UNCorpus.test.tok.lc:      |   19169     |   581  |     |       60.6048            |
----------------------------------------------------------------------------
UNCorpus.test.tok.lc.port: |   19169    |   4372 |        |       155.9641           |
----------------------------------------------------------------------------
UNCorpus.test.tok.lc.bpe:  |   19169     |  6544  |         |    165.822             |
----------------------------------------------------------------------------
----------------------------------------------------------------------------
Bible.test:                |   11995     |   4051      |         |      1816.905            |
----------------------------------------------------------------------------
Bible.test.tok:            |  13999      |  3053       |         |    1479.676              |
----------------------------------------------------------------------------
Bible.test.tok.lc:         |  13999      |  2969       |         |     1090.573             |
 ---------------------------------------------------------------------------
Bible.test.tok.lc.port:    |   13999     |   3900      |         |    866.579              |
----------------------------------------------------------------------------
Bible.test.tok.lc.bpe:     |  13999      |   4853      |         |     959.654             |
----------------------------------------------------------------------------
----------------------------------------------------------------------------
Fair.test:                 |  4388      |   1966      |         |   2235.367               |
----------------------------------------------------------------------------
Fair.test.tok:             |  19685     |   5661      |         |     1094.001             |
----------------------------------------------------------------------------
Fair.test.tok.lc:          | 19685      |   5575      |         |  1209.677                |
----------------------------------------------------------------------------
Fair.test.tok.lc.port:     |  19685    |   5873      |         |     1128.874             |
----------------------------------------------------------------------------
Fair.test.tok.lc.bpe:      |  19685    |     6053    |         |      1376.518            |
----------------------------------------------------------------------------


--------------------------------------------------------------------------------
Question 2.1.b (2 points).  What can you say about the interaction between
tokenizations and OOV rate for in-domain and out-of-domain cases?

OOV is much higher in out-of-domain cases

--------------------------------------------------------------------------------
Question 2.1.c (2 points).  What can you say about the interaction between
tokenizations and perplexity?

it is very high in out-of-domain cases which will reflect badly on the score/accuracy

--------------------------------------------------------------------------------
Question 2.1.d (2 points).  What do the results above suggest about the
similariy between the Bible and UN, vs My Fair Lady and the UN?


the vocabulary are much diffrent that UN in both cases




Question 2.2.a (4 points). Fill in the below table.



*****************************************************************************
                 |  Train Size |  Words  |   OOV   |  OOV%   |     ppl      |
*****************************************************************************
UNCorpus.test    |   70000     |    4332     |         |         |              |
-----------------------------------------------------------------------------
UNCorpus.test    |   35000     |     2312    |         |         |              |
-----------------------------------------------------------------------------
UNCorpus.test    |   17500     |         |         |         |              |
-----------------------------------------------------------------------------
UNCorpus.test    |    8750     |         |         |         |              |
-----------------------------------------------------------------------------
UNCorpus.test    |    4375     |         |         |         |              |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
Bible.test:      |   70000     |         |         |         |              |
-----------------------------------------------------------------------------
Bible.test:      |   35000     |         |         |         |              |
-----------------------------------------------------------------------------
Bible.test:      |   17500     |         |         |         |              |
-----------------------------------------------------------------------------
Bible.test:      |    8750     |         |         |         |              |
-----------------------------------------------------------------------------
Bible.test:      |    4375     |         |         |         |              |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
Fair.test:       |   70000     |         |         |         |              |
-----------------------------------------------------------------------------
Fair.test:       |   35000     |         |         |         |              |
-----------------------------------------------------------------------------
Fair.test:       |   17500     |         |         |         |              |
-----------------------------------------------------------------------------
Fair.test:       |    8750     |         |         |         |              |
-----------------------------------------------------------------------------
Fair.test:       |    4375     |         |         |         |              |
-----------------------------------------------------------------------------
-------------------------------------------------------------------------------
Question 2.2.b (2 points).  What can you say about the relationship between the
training size and OOV rate?




--------------------------------------------------------------------------------
Question 2.2.c (2 points).  What can you say about the relationship between the
training size and perplexity for in domain data?


--------------------------------------------------------------------------------
Question 2.2.d (2 points).  What can you say about the relationship between the
training size and perplexity for out-of-domain data?   Is it similar or
different to in-domain data?




--------------------------------------------------------------------------------
Question 2.3.a (4 points). Fill in the below table.



*****************************************************************************
                 |             |  Words  |   OOV   |  OOV%   |     ppl      |
*****************************************************************************
UNCorpus.test    | Order 1.lm  |         |         |         |              |
-----------------------------------------------------------------------------
UNCorpus.test    | Order 2.lm  |         |         |         |              |
-----------------------------------------------------------------------------
UNCorpus.test    | Order 3.lm  |         |         |         |              |
-----------------------------------------------------------------------------
UNCorpus.test    | Order 4.lm  |         |         |         |              |
----------------------------------------------------------------------------
UNCorpus.test    | Order 5.lm  |         |         |         |              |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
Bible.test:      | Order 1.lm  |         |         |         |              |
-----------------------------------------------------------------------------
Bible.test:      | Order 3.lm  |         |         |         |              |
-----------------------------------------------------------------------------
Bible.test:      | Order 3.lm  |         |         |         |              |
-----------------------------------------------------------------------------
Bible.test:      | Order 4.lm  |         |         |         |              |
-----------------------------------------------------------------------------
Bible.test:      | Order 5.lm  |         |         |         |              |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
Fair.test:       | Order 1.lm  |         |         |         |              |
-----------------------------------------------------------------------------
Fair.test:       | Order 2.lm  |         |         |         |              |
-----------------------------------------------------------------------------
Fair.test:       | Order 3.lm  |         |         |         |              |
-----------------------------------------------------------------------------
Fair.test:       | Order 4.lm  |         |         |         |              |
-----------------------------------------------------------------------------
Fair.test:       | Order 5.lm  |         |         |         |              |
-----------------------------------------------------------------------------




-------------------------------------------------------------------------------
Question 2.3.b (2 points).  What can you say about the relationship between
the LM order and OOV rate?



-------------------------------------------------------------------------------
Question 2.3.c (2 points).  What can you say about the relationship between the
LM order and perplexity for in domain data?


-------------------------------------------------------------------------------
Question 2.3.d (2 points).  What can you say about the relationship between the
LM order and perplexity for out-of-domain data?   Is it similar or different to
in-domain data?






--------------------------------------------------------------------------------
Question 2.4.a (4 points). Fill in the below table.



*****************************************************************************
                 | Smoothing   |  Words  |   OOV   |  OOV%   |     ppl      |
*****************************************************************************
UNCorpus.test    | Add 1       |         |         |         |              |
-----------------------------------------------------------------------------
UNCorpus.test    | Add 0.1     |         |         |         |              |
-----------------------------------------------------------------------------
UNCorpus.test    | Good-Turing |         |         |         |              |
-----------------------------------------------------------------------------
UNCorpus.test    | Witten-Bell |         |         |         |              |
-----------------------------------------------------------------------------
UNCorpus.test    | Kneser-Ney  |         |         |         |              |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
Bible.test:      | Add 1       |         |         |         |              |
-----------------------------------------------------------------------------
Bible.test:      | Add 0.1     |         |         |         |              |
-----------------------------------------------------------------------------
Bible.test:      | Good-Turing |         |         |         |              |
-----------------------------------------------------------------------------
Bible.test:      | Witten-Bell |         |         |         |              |
-----------------------------------------------------------------------------
Bible.test:      | Kneser-Ney  |         |         |         |              |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
Fair.test:       | Add 1       |         |         |         |              |
-----------------------------------------------------------------------------
Fair.test:       | Add 0.1     |         |         |         |              |
-----------------------------------------------------------------------------
Fair.test:       | Good-Turing |         |         |         |              |
-----------------------------------------------------------------------------
Fair.test:       | Witten-Bell |         |         |         |              |
-----------------------------------------------------------------------------
Fair.test:       | Kneser-Ney  |         |         |         |              |
-----------------------------------------------------------------------------


------------------------------------------------------------------------------
Question 2.4.b (2 points).  What can you say about the relationship between the
smoothing method and perplexity for in domain data?



-------------------------------------------------------------------------------
Question 2.4.c (2 points).  What can you say about the relationship between the
smoothing method and perplexity for out-of-domain data?
Is it similar or different to in-domain data?






--------------------------------------------------------------------------------
Question 2.4.d (2 points).  What is the best smoothing method overall?


================================================================================
================================================================================
Task 3: Guess The Language! (40 points)


1. To train on the train data and create a model directory that houses all
the .lm files:


python identify-language.py TRAIN Europarl/train/train modelxid


1. To predict the answers for the dev set and print them to STDIN/file


python identify-language.py PREDICT Europarl/dev modelxid > dev.predict


1. To evaluate the predictions against the gold


python identify-language.py EVALUATE Europarl/dev/dev.gold dev.predict



-------------------------------------------------------------------------------
Question 3.b (5 points) Modify your code to train with one line only from
training, and to use one line only from the dev files, and to use order 1
for lm.


* Run your system using this training data and report the results on the dev
set.  (1 point)



* It is reasonable to expect the accuracy to go down. Is there a pattern to
the errors?  (4 points)



--------------------------------------------------------------------------------
Question 3.c (15 points) Using your best model you have, identify the languages
of the provided test set (Europarl/test).  Provide your answer in a file named
test.pred. The file should consist of two tab separated columns marking for
each file test.<n>, its two-character language id.


test.1<tab><lang>
        test.2<tab><lang>
        ...
test.15<tab><lang>


The format should be comparable to Europarl/dev.gold.


The points for this question will be based on how many labels your system
assigns correctly.
*******************************************************************************
***********************************END*****************************************
*******************************************************************************
